---
title: "Journal (reproducible report)"
author: "Lars Behrendt"
date: "2020-12-01"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```


# Challenge 1 - Bike Sales Analysis
Last compiled: `r Sys.Date()`

The goal of the first challenge is to analyze the sales data of bike manufacturer Canyon for bikes sold in Germany. Two different aspects are of interest: first the sales by state and secondly the sales by state and year.

## Loading Librarys
First the libraries used for this challenge need to be loaded. In this case the whole tidyverse, readxl for file operations with the excel data and lubridate are loaded.

```{r eval=TRUE, include = TRUE, message = FALSE,warning=FALSE}
# 1.0 Load libraries ----
library(tidyverse)
library(readxl) # for Excel Data
library(lubridate)
```

## Importing the Sales Data
The Sales data is stored in Excel files. the library readxl is used to import the data. The data is loaded as a tibble.
```{r eval=TRUE, include = TRUE, message = FALSE,warning=FALSE}
# 2.0 Importing Files ----
bikes <- read_excel("J:/OneDrive/Studium/Masterstudium/DataScience/DS_101/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
bikeshops <- read_excel("J:/OneDrive/Studium/Masterstudium/DataScience/DS_101/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")
orderlines <- read_excel("J:/OneDrive/Studium/Masterstudium/DataScience/DS_101/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")
```

## Preparing Data 
After Import the sales data must be prepared for the analysis. First the data of the individual files needs to be joined together into one big tibble. The link between the datasets is done via their primary keys. After that the data is wrangled in the way needed for the analysis.
```{r eval=TRUE, include = TRUE, message = FALSE,warning=FALSE}
# 3.0 Joining Data ----
bikeData <- orderlines %>% left_join(bikes, by = c("product.id"="bike.id")) %>% left_join(bikeshops, by = c("customer.id" = "bikeshop.id"))

# 4.0 Wrangling Data ----

#Splitting category into individual sub categories
bikeData_Wrangled <- bikeData %>% separate(col= category,into = c("category.1", "category.2", "category.3"), sep =" - ") %>% 
  #Add total price
  mutate(total.price =price * quantity) %>% 
  select(-...1, -gender) %>%
  select(order.id, contains("order"), contains("model"), contains("category"),
         price, quantity, total.price,
         everything()) %>%
  set_names(names(.) %>% str_replace_all("\\.", "_"))

#Splitting Location into City and State
bikeData_Wrangled_challenge <- bikeData_Wrangled %>% separate(col = location, sep=",", into = c("city", "state")) 

```

## Analysis of the Sales Data
Now the data is ready to be analyzed. From the big tibble, which contains all the sales data, the needed information are selected and grouped according to the investigated aspect of interest.

### Sales by location
Fist the sales by location are analyzed.

```{r eval=TRUE, include = TRUE, message = FALSE,warning=FALSE}
# 5.0 Business Insights ----

# 5.1 Sales by Location ----

# Manipulate Data
salesbyLocation <- bikeData_Wrangled_challenge %>% mutate(year = year(order_date)) %>% select(year,total_price,state) %>% group_by(state) %>%
  summarise(sales = sum(total_price)) %>% mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                                                             decimal.mark = ",", 
                                                                             prefix = "", 
                                                                             suffix = " €"))
salesbyLocation
```
The results are then visualized.

```{r plot, fig.width=15, fig.height=10}
# Visualize Sales by Location
salesbyLocation %>% ggplot(aes(x = state, y=sales)) +
  geom_col(fill = "#00cccc") +
  geom_label(aes(label = sales_text)) +
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title    = "Revenue by state",
    x = "", # Override defaults for x and y
    y = "Revenue"
  ) + theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
One can clearly see that there are huge differences in sales revenues across the different states in Germany. By far the largest revenue was created in North-Rhine-Westphalia. Despite being one of the smaller states, Bremen has a very high revenue as well. Especially if you compare Bremen with the other city-states Hamburg and Berlin. Berlins revenue seems to be rather low considering the huge population of Berlin.

### Sales by location and year
The procedure for the second analysis is the same as before. Starting with manipulating the data in the way needed:
```{r eval=TRUE, include = TRUE, message = FALSE,warning=FALSE}

# 5.2 sales by location and year ----

#manipulate Data
salesByLocationAndYear <- bikeData_Wrangled_challenge %>% mutate(year = year(order_date)) %>% select(year,total_price,state) %>% group_by(state,year) %>%
  summarise(sales = sum(total_price)) %>% mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                                                             decimal.mark = ",", 
                                                                             prefix = "", 
                                                                             suffix = " €"))
salesByLocationAndYear
```

The results can than be visualized:

```{r plot2, fig.width=10, fig.height=7}

#Visualize sales by location and year
salesByLocationAndYear %>%  ggplot(aes(x = year, y=sales, fill= state)) +
  geom_col() +
  facet_wrap(~state) +
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title    = "Revenue by year and state",
    x = "Year", # Override defaults for x and y
    y = "Revenue",
    fill = "state"
  ) + theme(legend.position = "bottom") +
  geom_smooth(method = "lm" , se = F) # adding trendline

```

The results show that in most states the revenue has an upward trend. In some states revenue is stagnating or decreasing slightly.


# Challenge 2 API & Web scraping

## Gathering data via API

The First task in this challenge is to gather some data via an API. I chose the Spotify API to pull the newest album releases on Spotify in Germany. The Spotify API requires Authorization and therefore the the keys used for Authentification are not included in the code.

### loading Libraries and Authorization for Spotify API and pulling Data via API

```{r eval=TRUE, include = TRUE, message = FALSE,warning=FALSE}
#Challenge 2 API and Webscraping

#libraries
library(tidyverse)
library(rvest)
library(xopen)
library(jsonlite)
library(glue)
library(stringr)
library(httr)
library(purrr)

```

```{r eval=TRUE, include = FALSE, message = FALSE,warning=FALSE}
clientID = '382c22fa23294b0e9932248d3f3f5c96'
secret = 'd34057e26ac945e3836b1e238f5c83c2'
```

```{r eval=TRUE, include = TRUE, message = FALSE,warning=FALSE}
#API -> newest releases on Spotify in Germany
response = POST(
  'https://accounts.spotify.com/api/token',
  accept_json(),
  authenticate(clientID, secret),
  body = list(grant_type = 'client_credentials'),
  encode = 'form',
  verbose()
)
mytoken = content(response)$access_token

HeaderValue = paste0('Bearer ', mytoken)
resp <- GET(url = "https://api.spotify.com/v1/browse/new-releases?country=DE&limit=50", config = add_headers(authorization = HeaderValue))
```
### Manipulating Data and presenting result
After the data is pulled from the API it has to be manipulated because there are lots of information that are not of interest in this case.
The information I want to be displayed are the name of the album, the name of the artist, release Date of the album and the number of tracks in the album.

```{r eval=TRUE, include = TRUE, message = FALSE,warning=FALSE}
releaseData <- resp$content %>% rawToChar() %>% fromJSON()
albums <- releaseData$albums$items

#getArtist -> only gets First artist to avoid problems with dimensions of columns

firstArtist <- function(index){
  art <- albums$artists[[index]][1,4]
}
artistName <- unlist(map(seq_along(albums$artists),firstArtist))


albData<- tibble(name = albums$name, artist = artistName, releaseDate = albums$release_date, numberOfTracks = albums$total_tracks)

albData %>% print(n= 10)
```
The result shows the latest releases on Spotify Germany.

## Web scraping

The second task was to scrape the website of one of canyons competitors and gather information about the different bikes offered. The database should contain model names and prices.
I chose to scrape the Site of Rose Bikes. Since they offer quite a lot of different models I decided to also include the bike category in the database for a better overview.

I also scrapped all bike categories available. The css selectors used in the code were obtained by investigating the source code of the Rose Bikes website. All the necessary libraries where already included in the API code above.
### Getting URLs for the different categories

```{r eval=TRUE, include = TRUE, message = FALSE,warning=FALSE}

#Webscraping

url_base <- "https://www.rosebikes.de"
url_home <- "https://www.rosebikes.de/fahrr%C3%A4der/mtb"



#get Categories

html_base <- read_html(url_base)
bike_categories_selection <- html_base %>% 
  html_nodes(css = ".main-navigation__list:first-child") %>%
  html_nodes(css = ".main-navigation-category-with-tiles__link")
bike_categories_url <- bike_categories_selection %>%
  html_attr("href") %>% enframe(name ="position", value ="url")
bikes_categories_name <- bike_categories_selection %>%
  html_text() %>% str_replace_all(pattern = "\\n","")%>% enframe(name="position", value ="name")
bikes_categories <- left_join(bikes_categories_name,bike_categories_url)

bikes_categories <- subset(bikes_categories, name != "Sale") #delete sale items
bikes_categories

```
Now the Urls and names for all main categories are gathered. Each bike category consists of different bike families. The next step is to get the Urls for the individual bike families across all main categories.
``````{r eval=TRUE, include = TRUE, message = FALSE,warning=FALSE}

#get Urls for families in the categories

getFamilyUrls <- function(category_url,category_name){
  cat_url <- glue("{url_base}{category_url}")
  html_cat <- read_html(cat_url)
  bike_family_urls <- html_cat %>% html_nodes(css = ".catalog-category-bikes__button" ) %>%
    html_attr('href') %>% 
    enframe(name = "position", value = "url") %>% mutate(bike_category = category_name) %>% select(url,bike_category)
}


famUrls <- getFamilyUrls(bikes_categories$url[1], bikes_categories$name[1])

#iterate through the categories and append
for(i in 2:nrow(bikes_categories)){
  famUrls <-bind_rows(famUrls,getFamilyUrls(bikes_categories$url[i],bikes_categories$name[i]))
}

```
The URLs for the bike families are stored together with the name of the main category to enable me to store the category into the database later.

With the URLs for the bike families I can finally gather the information of the individual bike models.

``````{r eval=TRUE, include = TRUE, message = FALSE,warning=FALSE}
#get Bike urls

getBikeData <- function(model_cat_url, model_cat_name){
  bike_cat_url <- glue("{url_base}{model_cat_url}") 
  
  bike_cat_html <-read_html(bike_cat_url)
  
  bike_Names <- bike_cat_html %>% 
    html_nodes(css = ".catalog-category-model__title") %>%
    html_text() %>% str_replace_all(pattern = "\\n","")%>% enframe(name ="position", value ="model")
  bikePrices <- bike_cat_html %>%
    html_nodes(css = ".catalog-category-model__price-current-value") %>%
    html_text() %>% str_replace_all(pattern = "\\n","")%>% enframe(name ="position", value = "price")
  
  bikes_Data <- left_join(bike_Names,bikePrices, by= "position") %>% mutate(bike_category = model_cat_name) %>% select(model, price, bike_category)
}


#get Name and price for all bikes
bikeDataAll <- getBikeData(famUrls$url[1], famUrls$bike_category[1])

for(i in 2:nrow(famUrls)){
  bikeDataAll <- bind_rows(bikeDataAll, getBikeData(famUrls$url[i],famUrls$bike_category[i]))
}

print(bikeDataAll, n = Inf)


```
The results show all individual bike models and their prices as well as the main category the belong to.